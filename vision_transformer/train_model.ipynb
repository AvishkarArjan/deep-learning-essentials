{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Also checkout\n",
    "Pytorch paper replication  : https://www.learnpytorch.io/08_pytorch_paper_replicating/\"\"\"\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CONFIG\"\"\"\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(\"/content/drive/MyDrive/research\")\n",
    "CHECKPOINT_PATH = DATA_ROOT/\"vision_transformer/model_checkpoints\"\n",
    "RESUME=False\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NUM_EPOCHS = 300\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DATASET\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "USEFULL STUFF\n",
    "Pathlib - https://pythonandvba.com/wp-content/uploads/2021/11/Pathlib_Cheat_Sheet.pdf\n",
    "Caltech256 Dataset : https://pytorch.org/vision/stable/generated/torchvision.datasets.Caltech256.html#torchvision.datasets.Caltech256\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def to_rgb(image):\n",
    "  \"\"\"Converts a grayscale image to RGB format.\"\"\"\n",
    "  if len(image.getbands()) == 1:\n",
    "    # Add two dummy channels to make it RGB\n",
    "    return image.convert('RGB')\n",
    "  else:\n",
    "    return image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.Lambda(to_rgb),\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),           # Convert images to tensors\n",
    "])\n",
    "\n",
    "# dataset = datasets.Caltech256(DATA_ROOT, transform=transform, download=True)\n",
    "dataset = datasets.Caltech101(DATA_ROOT, transform=transform, download=True)\n",
    "print(\"Dataset size : \",len(dataset))\n",
    "indices = list(range(len(dataset)))\n",
    "\n",
    "split = int(0.8 * len(dataset))\n",
    "train_indices, test_indices = indices[:split], indices[split:]\n",
    "\n",
    "# Create training and test subsets using Subset\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MODEL\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "  def __init__(self, img_size, patch_size, in_channel=3, embed_dim=768):\n",
    "    super().__init__()\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_patches = (img_size//patch_size)**2\n",
    "\n",
    "    self.proj = nn.Conv2d(\n",
    "      in_channel,\n",
    "      embed_dim,\n",
    "      kernel_size=patch_size,\n",
    "      stride=patch_size\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    x = self.proj(x)\n",
    "    x = x.flatten(2)\n",
    "    x = x.transpose(1,2)\n",
    "\n",
    "    return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "  def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "    super().__init__()\n",
    "    self.n_heads=n_heads\n",
    "    self.dim=dim\n",
    "    self.head_dim= dim//n_heads\n",
    "    self.scale=self.head_dim** -0.5\n",
    "\n",
    "    self.qkv = nn.Linear(dim, dim*3, bias=qkv_bias)\n",
    "    self.attn_drop = nn.Dropout(attn_p)\n",
    "    self.proj = nn.Linear(dim, dim)\n",
    "    self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch, n_tokens, dim = x.shape\n",
    "    # assert dim != self.dim , \"chud gaya\"\n",
    "    if dim != self.dim:\n",
    "      raise ValueError\n",
    "\n",
    "\n",
    "    qkv = self.qkv(x)\n",
    "    qkv = qkv.reshape(batch, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "    qkv = qkv.permute(2,0,3,1,4)\n",
    "    q,k,v = qkv[0], qkv[1], qkv[2]\n",
    "    k_t = k.transpose(-2,-1)\n",
    "    dp = (\n",
    "      q@k_t\n",
    "    )*self.scale\n",
    "    attn = dp.softmax(dim=-1)\n",
    "    attn = self.attn_drop(attn)\n",
    "    weighted_avg = attn@v\n",
    "    weighted_avg = weighted_avg.transpose(1,2)\n",
    "    weighted_avg = weighted_avg.flatten(2)\n",
    "\n",
    "    x = self.proj(weighted_avg)\n",
    "    x = self.proj_drop(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "  def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "    self.act = nn.GELU()\n",
    "    self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "    self.drop = nn.Dropout(p)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.fc1(x)\n",
    "    x= self.act(x)\n",
    "    x = self.drop(x)\n",
    "    x = self.fc2(x)\n",
    "    x= self.drop(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0, attn_p=0.):\n",
    "    super().__init__()\n",
    "    self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
    "    self.attn = Attention(\n",
    "      dim,\n",
    "      n_heads=n_heads,\n",
    "      qkv_bias=qkv_bias,\n",
    "      attn_p=attn_p,\n",
    "      proj_p=p\n",
    "    )\n",
    "    self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
    "    hidden_features = int(dim*mlp_ratio)\n",
    "    self.mlp = MLP(\n",
    "      in_features=dim,\n",
    "      hidden_features=hidden_features,\n",
    "      out_features=dim\n",
    "    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x + self.attn(self.norm1(x))\n",
    "    x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "    return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    n_classes=1000,\n",
    "    embed_dim=768,\n",
    "    depth=12,\n",
    "    n_heads=12,\n",
    "    mlp_ratio=4,\n",
    "    qkv_bias=True,\n",
    "    p=0.,\n",
    "    attn_p=0.,\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.patch_embed = PatchEmbed(\n",
    "      img_size=img_size,\n",
    "      patch_size=patch_size,\n",
    "      in_channel=in_channels,\n",
    "      embed_dim=embed_dim\n",
    "    )\n",
    "    self.cls_token = nn.Parameter(torch.zeros(1,1, embed_dim))\n",
    "    self.pos_embed = nn.Parameter(\n",
    "      torch.zeros(1,1+self.patch_embed.n_patches, embed_dim)\n",
    "    )\n",
    "    self.pos_drop = nn.Dropout(p=p)\n",
    "\n",
    "    self.blocks = nn.ModuleList(\n",
    "      [\n",
    "        TransformerBlock(dim=embed_dim,\n",
    "             n_heads=n_heads,\n",
    "             mlp_ratio=mlp_ratio,\n",
    "             qkv_bias=qkv_bias,\n",
    "             p=p,\n",
    "             attn_p=attn_p\n",
    "             )\n",
    "        for _ in range(depth)\n",
    "      ]\n",
    "    )\n",
    "\n",
    "    self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "    self.head = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    batch_size = x.shape[0]\n",
    "    x = self.patch_embed(x)\n",
    "    cls_token = self.cls_token.expand(batch_size, -1,-1) # (batch, 1, embed_dim)\n",
    "    x = torch.cat((cls_token, x), dim=1)\n",
    "    x = x + self.pos_embed\n",
    "    x = self.pos_drop(x)\n",
    "\n",
    "\n",
    "    for block in self.blocks:\n",
    "      x= block(x)\n",
    "\n",
    "\n",
    "    x = self.norm(x)\n",
    "    cls_token_final = x[:, 0]# just cls token\n",
    "    x = self.head(cls_token_final)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Download ImageNet and train\n",
    "ImageNet Dataset Download : https://image-net.org/challenges/LSVRC/2012/2012-downloads.php\n",
    "ImageNet Dataset Download : https://image-net.org/download-images\n",
    "PyTorch ImageNet usage : https://pytorch.org/vision/main/generated/torchvision.datasets.ImageNet.html\n",
    "create config.py\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def save_checkpoint(state_dict, epoch, loss, path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Creating folder\")\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_details = {\n",
    "        \"epoch\":epoch,\n",
    "        \"state_dict\": state_dict,\n",
    "        \"loss\" : loss,\n",
    "    }\n",
    "    torch.save(model_details, f\"{p}/vit{epoch}.pth\")\n",
    "    print(f\"model saved at path : {p}/vit{epoch}.pth\")\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, epoch):\n",
    "    model.load_state_dict(torch.load(f\"{path}/vit{epoch}.pth\")[\"state_dict\"])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "  model = VisionTransformer()\n",
    "  if CHECKPOINT_PATH.exists() and RESUME:\n",
    "    models = []\n",
    "    for file in CHECKPOINT_PATH.iterdir():\n",
    "      models.append(file.stem)\n",
    "    models.sort()\n",
    "    ckpt = models[-1]\n",
    "    epoch = ckpt[-1]\n",
    "    print(epoch)\n",
    "\n",
    "    model = load_pretrained(model, CHECKPOINT_PATH/f\"{ckpt}{epoch}.pth\")\n",
    "  model = model.to(DEVICE)\n",
    "  print(\"Num parameters of model : \", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "  print(\"_______________________\\nStarting Training\\n\")\n",
    "\n",
    "  for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Training for epoch : \", epoch+1)\n",
    "    start = time.time()\n",
    "    for i, (imgs, labels) in enumerate(train_loader):\n",
    "      \n",
    "      imgs = imgs.to(DEVICE)\n",
    "      labels = labels.to(DEVICE)\n",
    "      \n",
    "      prediction = model(imgs)\n",
    "      loss = criterion(prediction, labels)\n",
    "      print(loss)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if (i%50==0):\n",
    "        print(\"Saving checkpoint : \", i)\n",
    "        save_checkpoint(model.state_dict(), epoch+1, loss, CHECKPOINT_PATH)\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Epoch {epoch+1} Training time : {end-start}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
