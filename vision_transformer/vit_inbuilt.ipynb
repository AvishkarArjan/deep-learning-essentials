{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e85fa74a-25f4-4526-b61f-15096fee0f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT exists at :  /home/avishkar/Desktop/research\n",
      "ckpt exists at :  /home/avishkar/Desktop/projects/deep_learning_essentials/vision_transformer/vit_cifar_checkpoints\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_ROOT = Path(\"/content/drive/MyDrive/research/vision_transformer\")\n",
    "    CHECKPOINT_DIR = DATA_ROOT/\"vit_cifar_checkpoints\"\n",
    "    EXPERIMENT_DIR = DATA_ROOT/\"experiments\"\n",
    "except:\n",
    "    DATA_ROOT = Path.home()/\"Desktop/research\"\n",
    "    CHECKPOINT_DIR = Path.home()/\"Desktop/projects/deep_learning_essentials/vision_transformer/vit_cifar_checkpoints\"\n",
    "    EXPERIMENT_DIR = Path.home()/\"Desktop/projects/deep_learning_essentials/vision_transformer/experiments\"\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    DATA_ROOT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Created Data dir\")\n",
    "else:\n",
    "    print(\"DATA_ROOT exists at : \", DATA_ROOT)\n",
    "    \n",
    "if not CHECKPOINT_DIR.exists():\n",
    "    CHECKPOINT_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # CHECKPOINT_DIR.mkdir()/\n",
    "    print(\"created CKPT dir\")\n",
    "else:\n",
    "    print(\"ckpt exists at : \", CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31fd8ce9-5078-4dee-af04-1cbb93a60aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "trainset : 50000, testset : 10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"DATASET\"\"\"\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATASET = \"cifar10\"\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        transforms.Resize(32),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.RandomResizedCrop(size=(32, 32), scale=(0.8, 1.0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(DATA_ROOT, train=True, download=True, transform= transform)\n",
    "test_dataset = datasets.CIFAR10(DATA_ROOT, train=False, download=True, transform= transform)\n",
    "print(f\"trainset : {len(train_dataset)}, testset : {len(test_dataset)}\")\n",
    "classes = train_dataset.classes\n",
    "\n",
    "# trainset = torch.utils.data.Subset(train_dataset, list(range(5000)))\n",
    "# testset = torch.utils.data.Subset(test_dataset, list(range(1000)))\n",
    "\n",
    "# prepare\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6f1bc74-b15f-49a2-98ef-9897b99c9e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.9875..0.8094669].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"VISUALIZE DATA\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, (imgs, labels) in enumerate(train_loader):\n",
    "    img = imgs[0]\n",
    "    plt.imshow(img.T.cpu().numpy())\n",
    "    plt.show()\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffad08e3-4373-4dec-b277-abf97b2ad229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UTILS\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_checkpoint(state_dict, epoch, path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Creating folder\")\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_details = {\n",
    "        \"epoch\":epoch,\n",
    "        \"state_dict\": state_dict,\n",
    "    }\n",
    "    torch.save(model_details, f\"{p}/vit_cifar10_{epoch}.pth\")\n",
    "    print(f\"model saved at path : {p}/vit_cifar10_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, epoch):\n",
    "    model.load_state_dict(torch.load(f\"{path}/vit_cifar10_{epoch}.pth\")[\"state_dict\"])\n",
    "    return model\n",
    "\n",
    "def save_experiment(model, epoch, config, train_losses, test_losses, accuracies, path):\n",
    "    exp_data = {\n",
    "        \"train_losses\":train_losses,\n",
    "        \"test_losses\":test_losses,\n",
    "        \"accuracies\":accuracies,\n",
    "        \"epoch\":epoch,\n",
    "    }\n",
    "    exp_name = config[\"exp_name\"]\n",
    "    config_file = path/f\"{exp_name}\"/\"config.json\"\n",
    "    metrics_file = path/f\"{exp_name}\"/\"metrics.json\"\n",
    "    files = [config_file , metrics_file]\n",
    "    for file in files:\n",
    "        if file.exists():\n",
    "            print(f\"{file} exists\")\n",
    "        else:\n",
    "            file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            file.touch()\n",
    "            print(f\"{file} created\")\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(exp_data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    save_checkpoint(model.state_dict(), epoch, path/f\"{exp_name}\")\n",
    "\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    with open(path/f\"{exp_name}\"/\"metrics.json\", 'r') as file:\n",
    "      data = json.load(file)\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    accuracies=data[\"accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path/exp_name, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, accuracies, epoch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5086b3bf-6b0f-48da-a5a9-029f4123d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device, path=CHECKPOINT_DIR):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.path=path\n",
    "        self.exp_dir = EXPERIMENT_DIR\n",
    "\n",
    "    def train(self, train_loader, test_loader, num_epochs, save_model_every_n_epochs=0):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        accuracies = [] \n",
    "\n",
    "        for i in range(num_epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            accuracy, test_loss = self.evaluate(test_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != num_epochs:\n",
    "                print('\\tSave checkpoint at epoch', i+1)\n",
    "                save_checkpoint(self.model.state_dict(), i+1, train_losses, test_losses, accuracies, self.path)\n",
    "\n",
    "        save_experiment(self.model, num_epochs, config, train_losses, test_losses, accuracies, self.exp_dir)\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(train_loader):\n",
    "            imgs = imgs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(imgs)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()* len(imgs)\n",
    "\n",
    "        return total_loss / len(train_loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (imgs, labels) in enumerate(test_loader):\n",
    "                imgs = imgs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                predictions = self.model(imgs)\n",
    "                \n",
    "                loss = self.criterion(predictions, labels)\n",
    "                total_loss += loss.item() * len(imgs)\n",
    "\n",
    "                 # Calculate the accuracy\n",
    "                predictions = torch.argmax(predictions, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        avg_loss = total_loss / len(test_loader.dataset)\n",
    "        return accuracy, avg_loss\n",
    "\n",
    "config = {\n",
    "    \"img_size\":32,\n",
    "\t\"patch_size\":6,\n",
    "\t\"num_channels\":3,\n",
    "\t\"num_layers\":7,\n",
    "\t\"num_heads\":8,\n",
    "\t\"embed_dim\":768,\n",
    "\t\"mlp_hidden_dim\":4*768,\n",
    "\t\"dropout\":0.0,\n",
    "\t\"num_classes\":10,\n",
    "    \"lr\":0.01,\n",
    "    \"device\":\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"num_epochs\":100,\n",
    "    \"exp_name\":\"vit_cifar10_mark2_100_epochs\",\n",
    "    \"save_model_every\":0\n",
    "}\n",
    "\n",
    "def main():\n",
    "    save_model_every_n_epochs = config[\"save_model_every\"]\n",
    "    model = torchvision.models.vision_transformer.VisionTransformer(\n",
    "    image_size=32,\n",
    "    patch_size=4,\n",
    "    num_layers=8,\n",
    "    num_heads=4,\n",
    "    hidden_dim=768,\n",
    "    mlp_dim=4*768\n",
    ") \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=1e-2)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model, optimizer, criterion, device=config[\"device\"])\n",
    "    trainer.train(train_loader, test_loader, config[\"num_epochs\"], save_model_every_n_epochs=save_model_every_n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d63ece2-b4df-4f22-8300-b4501a257c04",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"TRAINING\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 100\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     99\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optimizer, criterion, device\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 100\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model_every_n_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model_every_n_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, test_loader, num_epochs, save_model_every_n_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m [] \n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 19\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     accuracy, test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(test_loader)\n\u001b[1;32m     21\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[18], line 41\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(imgs)\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(predictions, labels)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(imgs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"TRAINING\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d19adcd-28c2-40a0-95f8-d32db0b6d61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
