{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034a5256-f08d-406f-8acb-3e05f6f8ec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to get data\n",
      "Trainset: 60000 , Testset : 10000\n",
      "torch.Size([32, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_DIR = Path(\"/content/mnist_data\")\n",
    "    EXP_DIR = Path(\"/content/drive/MyDrive/research/experiments\")\n",
    "except:\n",
    "    DATA_DIR = Path(\"/home/avishkar/Desktop/research/mnist_data\")\n",
    "    EXP_DIR = Path(\"/home/avishkar/Desktop/research/experiments\")\n",
    "    print(\"About to get data\")    \n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomRotation(7),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "trainset = MNIST(root=DATA_DIR, train=True, download=True, transform= transform)\n",
    "testset = MNIST(root=DATA_DIR, train=False, download=True, transform = transform)\n",
    "print(f\"Trainset: {len(trainset)} , Testset : {len(testset)}\")\n",
    "classes = trainset.classes\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=32, num_workers=2, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=32, num_workers = 2, shuffle=False)\n",
    "\n",
    "\"\"\"VISUALIZE DATA\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, (imgs, labels) in enumerate(train_loader):\n",
    "    print(imgs.shape)\n",
    "    img = imgs[0]\n",
    "    plt.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5077d5-e454-4eab-aa85-382b683dadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.img_size = config[\"img_size\"]\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_patches = (self.img_size // self.patch_size) **2\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.num_channels,\n",
    "                out_channels=self.embed_dim,\n",
    "                kernel_size=self.patch_size,\n",
    "                stride=self.patch_size,\n",
    "            ),\n",
    "            nn.Flatten(2)\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(size=(1, 1,self.embed_dim)),\n",
    "            requires_grad=True\n",
    "            )\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.randn(size=(1, self.num_patches+1, self.embed_dim)),\n",
    "            requires_grad=True\n",
    "            )\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        \n",
    "        x = self.patcher(x).permute(0, 2, 1)\n",
    "        x = torch.cat([cls_token,x ], dim=1)\n",
    "        x = self.position_embeddings + x\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        \n",
    "        self.embeddings = PatchEmbeddings(config)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dropout=config[\"dropout\"],\n",
    "            activation = \"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.encoder_blocks = nn.TransformerEncoder(\n",
    "            self.encoder_layer,\n",
    "            num_layers = config[\"num_layers\"]\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.embed_dim),\n",
    "            nn.Linear(self.embed_dim, self.num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :]) # apply MLP on the CLS token only\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494d106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UTILS\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_checkpoint(state_dict, epoch, path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Creating folder\")\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_details = {\n",
    "        \"epoch\":epoch,\n",
    "        \"state_dict\": state_dict,\n",
    "    }\n",
    "    torch.save(model_details, f\"{p}/vit_cifar10_{epoch}.pth\")\n",
    "    print(f\"model saved at path : {p}/vit_cifar10_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, epoch):\n",
    "    model.load_state_dict(torch.load(f\"{path}/vit_cifar10_{epoch}.pth\")[\"state_dict\"])\n",
    "    return model\n",
    "\n",
    "def save_experiment(model, epoch, config, train_losses, test_losses, train_accuracies,test_accuracies, path):\n",
    "    exp_data = {\n",
    "        \"train_losses\":train_losses,\n",
    "        \"test_losses\":test_losses,\n",
    "        \"train_accuracies\":train_accuracies,\n",
    "        \"test_accuracies\":test_accuracies,\n",
    "        \"epoch\":epoch,\n",
    "    }\n",
    "    exp_name = config[\"exp_name\"]\n",
    "    config_file = path/f\"{exp_name}\"/\"config.json\"\n",
    "    metrics_file = path/f\"{exp_name}\"/\"metrics.json\"\n",
    "    files = [config_file , metrics_file]\n",
    "    for file in files:\n",
    "        if file.exists():\n",
    "            print(f\"{file} exists\")\n",
    "        else:\n",
    "            file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            file.touch()\n",
    "            print(f\"{file} created\")\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(exp_data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    save_checkpoint(model.state_dict(), epoch, path/f\"{exp_name}\")\n",
    "\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    with open(path/f\"{exp_name}\"/\"metrics.json\", 'r') as file:\n",
    "      data = json.load(file)\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    train_accuracies=data[\"train_accuracies\"]\n",
    "    test_accuracies=data[\"test_accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path/exp_name, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, train_accuracies,test_accuracies, epoch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "844a9206-35fa-4209-ba71-4a1245e80fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config ={\n",
    "        \"img_size\":28,\n",
    "        \"embed_dim\":16, # (PATCH_SIZE ** 2) * IN_CHANNELS\n",
    "        \"patch_size\":4,\n",
    "        \"dropout\":0.01,\n",
    "        \"num_channels\":1,\n",
    "        \"num_heads\":4,\n",
    "        \"num_layers\":8,\n",
    "        \"num_classes\":10,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"exp_name\":\"vit_mnist_40_epoch\",\n",
    "        \"num_epoch\":40\n",
    "    }\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, device):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, train_loader, test_loader, num_epochs):\n",
    "        train_losses, test_losses, train_accuracies, test_accuracies = [], [] , [], []\n",
    "        start = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            ep_start = time.time()\n",
    "            train_loss, train_accuracy = self.train_epoch(train_loader)\n",
    "            test_loss, test_accuracy = self.evaluate(test_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "\n",
    "            ep_end = time.time()\n",
    "            print(f\"Epoch: {epoch}/{num_epochs}, Time : {(ep_end-ep_start):.2f}s\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Train Accuracy : {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "            \n",
    "        save_experiment(self.model, num_epochs, config, train_losses, test_losses, train_accuracies,test_accuracies, EXP_DIR)\n",
    "        end = time.time()\n",
    "        print(f\"Total Training Time : {(end-start):.2f}s\")\n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        correct = 0\n",
    "        running_train_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(tqdm(train_loader, position=0, leave=True)):\n",
    "            imgs = imgs.float().to(self.device)\n",
    "            labels = labels.type(torch.uint8).to(self.device)\n",
    "\n",
    "            predictions = self.model(imgs)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            running_train_loss += loss.item() * len(imgs)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "            correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        accuracy = correct / len(train_loader.dataset)\n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        return train_loss, accuracy\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        running_test_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(tqdm(test_loader, position=0, leave=True)):\n",
    "            imgs = imgs.float().to(self.device)\n",
    "            labels = labels.type(torch.uint8).to(self.device)\n",
    "\n",
    "            predictions = self.model(imgs)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            running_test_loss += loss.item() * len(imgs)\n",
    "\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "            correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        test_loss = running_test_loss / len(test_loader.dataset)\n",
    "        return test_loss, accuracy\n",
    "            \n",
    "def main():\n",
    "    model = ViT(config)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    trainer = Trainer(model, criterion, optimizer, config[\"device\"])\n",
    "    trainer.train(train_loader, test_loader, config[\"num_epoch\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8064f709-b40e-44f6-867f-b0d6d73f3136",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avishkar/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "  1%|█▏                                                                                      | 24/1875 [00:04<05:53,  5.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 94\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     93\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, criterion, optimizer, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 94\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     29\u001b[0m     ep_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 30\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(test_loader)\n\u001b[1;32m     32\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     56\u001b[0m running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(imgs)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 59\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(predictions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1e08f-70c0-47db-9dbe-7ba2536fab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize Losses\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = ViT(config)\n",
    "_, train_losses, test_losses, train_accuracies, test_accuracies,_ = load_experiment(model, config[\"exp_name\"], EXP_DIR)\n",
    "# Create two subplots of train/test losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(train_losses, label=\"Train loss\")\n",
    "ax1.plot(test_losses, label=\"Test loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax2.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "ax2.plot(test_accuracies, label=\"Test accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax1.legend()\n",
    "plt.savefig(\"metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc06e8-09cd-4e69-bdce-b542df1bb57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767e4e6-c97d-45d2-82bb-0f42d92a6212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d9b60-ee0a-4581-bdb0-fc01c1342d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273a0a8-6879-4228-9017-728d0382bd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e4e6c-3a4c-451d-b163-5a91c9c12629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509593ce-0d6c-4a99-8225-77b37e51d04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3dea5-b17e-461b-b135-2295fd12ef94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c10460-d222-49d3-a78c-b7f6c9119c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea482817-5a1b-42a5-b225-74475488ee15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbd4e4-3da9-4e03-a4fb-82f3e233e39e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
