{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "034a5256-f08d-406f-8acb-3e05f6f8ec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to get data\n",
      "Trainset: 60000 , Testset : 10000\n",
      "torch.Size([32, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcnElEQVR4nO3df3BUZb7n8U8HkhYkaQwx6WQITEABRyBeGYm5KOKQAmJdil9biz9qCywXLkywRPy1zKrozOyNg7cYrg6DtVszMO4KKHeFXNkrWxhMWMeAlwjLOM5kSYwCAwnKXbpDkBCSZ/9gbW1J0NN259sJ71fVqaLPOd88Xx6PfHLSp5/4nHNOAAD0sBTrBgAAVyYCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACb6WzfwdZ2dnTp+/LjS09Pl8/ms2wEAeOScU0tLi/Ly8pSS0v19TtIF0PHjx5Wfn2/dBgDgOzp69KiGDh3a7fGkC6D09HRJ0m26S/2VatwNAMCrC2rXO/rnyL/n3UlYAK1bt07PP/+8mpqaVFhYqBdffFETJ078xrovfuzWX6nq7yOAAKDX+f8rjH7T2ygJeQjh1Vdf1YoVK7Rq1Sq9//77Kiws1PTp03Xy5MlEDAcA6IUSEkBr1qzRokWLdP/99+sHP/iBXnrpJQ0cOFC//e1vEzEcAKAXinsAnT9/XrW1tSopKflykJQUlZSUqKam5pLz29raFA6HozYAQN8X9wD67LPP1NHRoZycnKj9OTk5ampquuT88vJyBQKByMYTcABwZTD/IOrKlSsVCoUi29GjR61bAgD0gLg/BZeVlaV+/fqpubk5an9zc7OCweAl5/v9fvn9/ni3AQBIcnG/A0pLS9OECRNUWVkZ2dfZ2anKykoVFxfHezgAQC+VkM8BrVixQgsWLNAPf/hDTZw4UWvXrlVra6vuv//+RAwHAOiFEhJA8+fP16effqqnn35aTU1Nuummm7Rz585LHkwAAFy5fM45Z93EV4XDYQUCAU3RLFZCAIBe6IJrV5UqFAqFlJGR0e155k/BAQCuTAQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADAR9wB65pln5PP5orYxY8bEexgAQC/XPxFf9MYbb9Rbb7315SD9EzIMAKAXS0gy9O/fX8FgMBFfGgDQRyTkPaDDhw8rLy9PI0aM0H333acjR450e25bW5vC4XDUBgDo++IeQEVFRdq4caN27typ9evXq7GxUbfffrtaWlq6PL+8vFyBQCCy5efnx7slAEAS8jnnXCIHOH36tIYPH641a9bogQceuOR4W1ub2traIq/D4bDy8/M1RbPU35eayNYAAAlwwbWrShUKhULKyMjo9ryEPx0wePBgjRo1SvX19V0e9/v98vv9iW4DAJBkEv45oDNnzqihoUG5ubmJHgoA0IvEPYAeffRRVVdX6+OPP9a7776rOXPmqF+/frrnnnviPRQAoBeL+4/gjh07pnvuuUenTp3Stddeq9tuu0179+7VtddeG++hAAC9WNwDaMuWLfH+kgCAPoi14AAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJhI+C+kA3Cplvm3eq/J9/79Yt7fv+u5Bugp3AEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEywGjbwFf1uuN5zTdGWP3quWXzN33uu+ejCQM81f/f6XM81knTho49jqoN0dk6R55qONJ/nmmveOeK5RpIu/OV4THWJwB0QAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEyxGih712eJizzVpLS4BnXRt/pM7PdeUDW7wXPOfQzd4rln7T3/juea6Tz/wXNMX9Q/mxFQ3cse/eq55Lvii55pUXz/PNatO/pXnGkmq/avkue9Ink4AAFcUAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJliMFDGLZWHR2mfWex+no9VzTSDlKs81kjS/YYbnml0zb/Jcc+Gjjz3XjFCN55pOzxU9y9ff+z9BZ/9Hvueat8f+d881khTuPOe55qb/tdRzTXvI77lm1JL3PNckG+6AAAAmCCAAgAnPAbRnzx7NnDlTeXl58vl82r59e9Rx55yefvpp5ebmasCAASopKdHhw4fj1S8AoI/wHECtra0qLCzUunXrujy+evVqvfDCC3rppZe0b98+XX311Zo+fbrOnfP+s1QAQN/l+R3A0tJSlZaWdnnMOae1a9fqySef1KxZsyRJL7/8snJycrR9+3bdfffd361bAECfEdf3gBobG9XU1KSSkpLIvkAgoKKiItXUdP0ET1tbm8LhcNQGAOj74hpATU1NkqScnOjfv56TkxM59nXl5eUKBAKRLT/f+yOWAIDex/wpuJUrVyoUCkW2o0ePWrcEAOgBcQ2gYDAoSWpubo7a39zcHDn2dX6/XxkZGVEbAKDvi2sAFRQUKBgMqrKyMrIvHA5r3759Ki72/ql5AEDf5fkpuDNnzqi+vj7yurGxUQcPHlRmZqaGDRum5cuX6+c//7muv/56FRQU6KmnnlJeXp5mz54dz74BAL2c5wDav3+/7rzzzsjrFStWSJIWLFigjRs36vHHH1dra6sWL16s06dP67bbbtPOnTt11VWxrc0FAOibfM45Z93EV4XDYQUCAU3RLPX3pVq30+ukpKd7rql/amxMY/2n2Zs816T5OjzXjEvr+gnKy1nwyCOeayTp6n/cF1MdYvPJa+M81/xh0kbPNSnyea6RpOt2/K3nmlF/+y8xjdWXXHDtqlKFQqHQZd/XN38KDgBwZSKAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmPD86xjQc1IGDvRcU/d3P/BeM/dXnmskqbnjc881CxY+5LnmXJb3VdEH/eNezzX40skf/7Xnmqv+b6fnmj9O+rXnGsWwsvXoqgdiGEca8+ifPdd4n4UrF3dAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATLAYaRI78tBNnmvq5r7ouSaWRUUlaf6jj3quGbTb+yKhgzxX4Asp6ekx1f3u8TWea9qd9+9nO9XPc80NVf/ec831S+o910hSZ0tLTHX4drgDAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYILFSJPYoWW/iqHK57lizqrHYhhHynytJqY6xKZ/MMdzzeRdH8U01o2paZ5rOuU814zduMxzzag1dZ5rOlhUNClxBwQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEi5H2Ma+0ZHuuyfjkfExj9bvh+pjqvOr40+EeGSfZffizYZ5r/inzzQR00rX5DTM814yMZWHRU//quQbJiTsgAIAJAggAYMJzAO3Zs0czZ85UXl6efD6ftm/fHnV84cKF8vl8UduMGd5vzQEAfZvnAGptbVVhYaHWrVvX7TkzZszQiRMnItvmzZu/U5MAgL7H80MIpaWlKi0tvew5fr9fwWAw5qYAAH1fQt4DqqqqUnZ2tkaPHq2lS5fq1KlT3Z7b1tamcDgctQEA+r64B9CMGTP08ssvq7KyUr/4xS9UXV2t0tJSdXR0dHl+eXm5AoFAZMvPz493SwCAJBT3zwHdfffdkT+PGzdO48eP18iRI1VVVaWpU6decv7KlSu1YsWKyOtwOEwIAcAVIOGPYY8YMUJZWVmqr6/v8rjf71dGRkbUBgDo+xIeQMeOHdOpU6eUm5ub6KEAAL2I5x/BnTlzJupuprGxUQcPHlRmZqYyMzP17LPPat68eQoGg2poaNDjjz+u6667TtOnT49r4wCA3s1zAO3fv1933nln5PUX798sWLBA69ev16FDh/S73/1Op0+fVl5enqZNm6af/exn8vv98esaANDr+ZxzzrqJrwqHwwoEApqiWervS7Vux1T92ls91/zh37zguSbV189zjST98fwFzzUpPu+XW9nDD3muOTY9tst64Cfen8sZ2Ox9rFN3tHmuOTC1+w9/d2egL81zjSSlyOe55ob/Wua5Jri366djL2fA9vc816BnXXDtqlKFQqHQZd/XZy04AIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJVsPuYz6fPdFzTUdabN+HHJ/S6bmm4i7vq3XfkJrc10EsK0d3qmf+t4tlxXJJmvM/H/Rc839mrvdc0+68r4b9+InbPdccWH2T5xpJGrR1X0x1VzpWwwYAJDUCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAm+ls3gPgasP29Hhtr1Gvea+79Dys81wQ+8r7oaaw+vdn7wqJ//He/8l4TwyKh//G2OZ5rLvzluOcaSRol79fRKC31XJP/pucSDXqn3nvNKRYVTUbcAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDBYqToUd977t0eGeezxcUx1f10zhbPNY81FXmu+cNjhZ5r+v+l1nNNTxq1pGcWwu3okVHQE7gDAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYILFSJH0fH6/55riRe/HNNYtVx31XPPy39zpuab/4eReWBToCdwBAQBMEEAAABOeAqi8vFy33HKL0tPTlZ2drdmzZ6uuri7qnHPnzqmsrExDhgzRoEGDNG/ePDU3N8e1aQBA7+cpgKqrq1VWVqa9e/dq165dam9v17Rp09Ta2ho55+GHH9Ybb7yhrVu3qrq6WsePH9fcuXPj3jgAoHfz9BDCzp07o15v3LhR2dnZqq2t1eTJkxUKhfSb3/xGmzZt0o9+9CNJ0oYNG3TDDTdo7969uvXWW+PXOQCgV/tO7wGFQiFJUmZmpiSptrZW7e3tKikpiZwzZswYDRs2TDU1NV1+jba2NoXD4agNAND3xRxAnZ2dWr58uSZNmqSxY8dKkpqampSWlqbBgwdHnZuTk6OmpqYuv055ebkCgUBky8/Pj7UlAEAvEnMAlZWV6YMPPtCWLVu+UwMrV65UKBSKbEePev8cBgCg94npg6jLli3Tjh07tGfPHg0dOjSyPxgM6vz58zp9+nTUXVBzc7OCwWCXX8vv98sfwwcNAQC9m6c7IOecli1bpm3btmn37t0qKCiIOj5hwgSlpqaqsrIysq+urk5HjhxRcXFxfDoGAPQJnu6AysrKtGnTJlVUVCg9PT3yvk4gENCAAQMUCAT0wAMPaMWKFcrMzFRGRoYefPBBFRcX8wQcACCKpwBav369JGnKlClR+zds2KCFCxdKkn75y18qJSVF8+bNU1tbm6ZPn65f//rXcWkWANB3+JxzzrqJrwqHwwoEApqiWervS7VuB0ng6JN/7bnmfy99Maaxbv6HBz3X5K1+N6axgL7qgmtXlSoUCoWUkZHR7XmsBQcAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMBHTb0QFetK9/3a355rNLTkxjcXK1kDP4Q4IAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACRYjRdJLkfNcs+rtuTGNNUrvxVQHwDvugAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJhgMVIkvTMdfs8136/wvoApgJ7FHRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATLEaKpPdhONdzTdrOf0lAJwDiiTsgAIAJAggAYMJTAJWXl+uWW25Renq6srOzNXv2bNXV1UWdM2XKFPl8vqhtyZIlcW0aAND7eQqg6upqlZWVae/evdq1a5fa29s1bdo0tba2Rp23aNEinThxIrKtXr06rk0DAHo/Tw8h7Ny5M+r1xo0blZ2drdraWk2ePDmyf+DAgQoGg/HpEADQJ32n94BCoZAkKTMzM2r/K6+8oqysLI0dO1YrV67U2bNnu/0abW1tCofDURsAoO+L+THszs5OLV++XJMmTdLYsWMj+++9914NHz5ceXl5OnTokJ544gnV1dXp9ddf7/LrlJeX69lnn421DQBAL+VzzrlYCpcuXao333xT77zzjoYOHdrtebt379bUqVNVX1+vkSNHXnK8ra1NbW1tkdfhcFj5+fmaolnq70uNpTX0MQOqczzXfH5HcwI6AfBtXHDtqlKFQqGQMjIyuj0vpjugZcuWaceOHdqzZ89lw0eSioqKJKnbAPL7/fL7/bG0AQDoxTwFkHNODz74oLZt26aqqioVFBR8Y83BgwclSbm53j/NDgDouzwFUFlZmTZt2qSKigqlp6erqalJkhQIBDRgwAA1NDRo06ZNuuuuuzRkyBAdOnRIDz/8sCZPnqzx48cn5C8AAOidPAXQ+vXrJV38sOlXbdiwQQsXLlRaWpreeustrV27Vq2trcrPz9e8efP05JNPxq1hAEDf4PlHcJeTn5+v6urq79QQAODKwGrYSHrH/tsIzzVDxFNwQLJjMVIAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmWIwUSW/If6mxbgFAAnAHBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATSbcWnHNOknRB7ZIzbgYA4NkFtUv68t/z7iRdALW0tEiS3tE/G3cCAPguWlpaFAgEuj3uc98UUT2ss7NTx48fV3p6unw+X9SxcDis/Px8HT16VBkZGUYd2mMeLmIeLmIeLmIeLkqGeXDOqaWlRXl5eUpJ6f6dnqS7A0pJSdHQoUMve05GRsYVfYF9gXm4iHm4iHm4iHm4yHoeLnfn8wUeQgAAmCCAAAAmelUA+f1+rVq1Sn6/37oVU8zDRczDRczDRczDRb1pHpLuIQQAwJWhV90BAQD6DgIIAGCCAAIAmCCAAAAmek0ArVu3Tt///vd11VVXqaioSO+99551Sz3umWeekc/ni9rGjBlj3VbC7dmzRzNnzlReXp58Pp+2b98eddw5p6efflq5ubkaMGCASkpKdPjwYZtmE+ib5mHhwoWXXB8zZsywaTZBysvLdcsttyg9PV3Z2dmaPXu26urqos45d+6cysrKNGTIEA0aNEjz5s1Tc3OzUceJ8W3mYcqUKZdcD0uWLDHquGu9IoBeffVVrVixQqtWrdL777+vwsJCTZ8+XSdPnrRurcfdeOONOnHiRGR75513rFtKuNbWVhUWFmrdunVdHl+9erVeeOEFvfTSS9q3b5+uvvpqTZ8+XefOnevhThPrm+ZBkmbMmBF1fWzevLkHO0y86upqlZWVae/evdq1a5fa29s1bdo0tba2Rs55+OGH9cYbb2jr1q2qrq7W8ePHNXfuXMOu4+/bzIMkLVq0KOp6WL16tVHH3XC9wMSJE11ZWVnkdUdHh8vLy3Pl5eWGXfW8VatWucLCQus2TEly27Zti7zu7Ox0wWDQPf/885F9p0+fdn6/323evNmgw57x9XlwzrkFCxa4WbNmmfRj5eTJk06Sq66uds5d/G+fmprqtm7dGjnnT3/6k5PkampqrNpMuK/Pg3PO3XHHHe6hhx6ya+pbSPo7oPPnz6u2tlYlJSWRfSkpKSopKVFNTY1hZzYOHz6svLw8jRgxQvfdd5+OHDli3ZKpxsZGNTU1RV0fgUBARUVFV+T1UVVVpezsbI0ePVpLly7VqVOnrFtKqFAoJEnKzMyUJNXW1qq9vT3qehgzZoyGDRvWp6+Hr8/DF1555RVlZWVp7NixWrlypc6ePWvRXreSbjHSr/vss8/U0dGhnJycqP05OTn685//bNSVjaKiIm3cuFGjR4/WiRMn9Oyzz+r222/XBx98oPT0dOv2TDQ1NUlSl9fHF8euFDNmzNDcuXNVUFCghoYG/eQnP1FpaalqamrUr18/6/birrOzU8uXL9ekSZM0duxYSRevh7S0NA0ePDjq3L58PXQ1D5J07733avjw4crLy9OhQ4f0xBNPqK6uTq+//rpht9GSPoDwpdLS0sifx48fr6KiIg0fPlyvvfaaHnjgAcPOkAzuvvvuyJ/HjRun8ePHa+TIkaqqqtLUqVMNO0uMsrIyffDBB1fE+6CX0908LF68OPLncePGKTc3V1OnTlVDQ4NGjhzZ0212Kel/BJeVlaV+/fpd8hRLc3OzgsGgUVfJYfDgwRo1apTq6+utWzHzxTXA9XGpESNGKCsrq09eH8uWLdOOHTv09ttvR/36lmAwqPPnz+v06dNR5/fV66G7eehKUVGRJCXV9ZD0AZSWlqYJEyaosrIysq+zs1OVlZUqLi427MzemTNn1NDQoNzcXOtWzBQUFCgYDEZdH+FwWPv27bvir49jx47p1KlTfer6cM5p2bJl2rZtm3bv3q2CgoKo4xMmTFBqamrU9VBXV6cjR470qevhm+ahKwcPHpSk5LoerJ+C+Da2bNni/H6/27hxo/vwww/d4sWL3eDBg11TU5N1az3qkUcecVVVVa6xsdH9/ve/dyUlJS4rK8udPHnSurWEamlpcQcOHHAHDhxwktyaNWvcgQMH3CeffOKcc+65555zgwcPdhUVFe7QoUNu1qxZrqCgwH3++efGncfX5eahpaXFPfroo66mpsY1Nja6t956y918883u+uuvd+fOnbNuPW6WLl3qAoGAq6qqcidOnIhsZ8+ejZyzZMkSN2zYMLd79263f/9+V1xc7IqLiw27jr9vmof6+nr305/+1O3fv981Nja6iooKN2LECDd58mTjzqP1igByzrkXX3zRDRs2zKWlpbmJEye6vXv3WrfU4+bPn+9yc3NdWlqa+973vufmz5/v6uvrrdtKuLfffttJumRbsGCBc+7io9hPPfWUy8nJcX6/302dOtXV1dXZNp0Al5uHs2fPumnTprlrr73WpaamuuHDh7tFixb1uW/Suvr7S3IbNmyInPP555+7H//4x+6aa65xAwcOdHPmzHEnTpywazoBvmkejhw54iZPnuwyMzOd3+931113nXvsscdcKBSybfxr+HUMAAATSf8eEACgbyKAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGDi/wEjfO+4iSppxQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_DIR = Path(\"/content/mnist_data\")\n",
    "    EXP_DIR = Path(\"/content/drive/MyDrive/research/experiments\")\n",
    "except:\n",
    "    DATA_DIR = Path(\"/home/avishkar/Desktop/research/mnist_data\")\n",
    "    EXP_DIR = Path(\"/home/avishkar/Desktop/research/experiments\")\n",
    "    print(\"About to get data\")    \n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomRotation(7),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "trainset = MNIST(root=DATA_DIR, train=True, download=True, transform= transform)\n",
    "testset = MNIST(root=DATA_DIR, train=False, download=True, transform = transform)\n",
    "print(f\"Trainset: {len(trainset)} , Testset : {len(testset)}\")\n",
    "classes = trainset.classes\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=32, num_workers=2, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=32, num_workers = 2, shuffle=False)\n",
    "\n",
    "\"\"\"VISUALIZE DATA\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, (imgs, labels) in enumerate(train_loader):\n",
    "    print(imgs.shape)\n",
    "    img = imgs[0]\n",
    "    plt.imshow(img.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5077d5-e454-4eab-aa85-382b683dadb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.img_size = config[\"img_size\"]\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_patches = (self.img_size // self.patch_size) **2\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.num_channels,\n",
    "                out_channels=self.embed_dim,\n",
    "                kernel_size=self.patch_size,\n",
    "                stride=self.patch_size,\n",
    "            ),\n",
    "            nn.Flatten(2)\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(size=(1, self.num_channels,self.embed_dim)),\n",
    "            requires_grad=True\n",
    "            )\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.randn(size=(1, self.num_patches+1, self.embed_dim)),\n",
    "            requires_grad=True\n",
    "            )\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        \n",
    "        x = self.patcher(x).permute(0, 2, 1)\n",
    "        x = torch.cat([cls_token,x ], dim=1)\n",
    "        x = self.position_embeddings + x\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        \n",
    "        self.embeddings = PatchEmbeddings(config)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dropout=config[\"dropout\"],\n",
    "            activation = \"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.encoder_blocks = nn.TransformerEncoder(\n",
    "            self.encoder_layer,\n",
    "            num_layers = config[\"num_layers\"]\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.embed_dim),\n",
    "            nn.Linear(self.embed_dim, self.num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :]) # apply MLP on the CLS token only\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "494d106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UTILS\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_checkpoint(state_dict, epoch, path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Creating folder\")\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_details = {\n",
    "        \"epoch\":epoch,\n",
    "        \"state_dict\": state_dict,\n",
    "    }\n",
    "    torch.save(model_details, f\"{p}/vit_cifar10_{epoch}.pth\")\n",
    "    print(f\"model saved at path : {p}/vit_cifar10_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, epoch):\n",
    "    model.load_state_dict(torch.load(f\"{path}/vit_cifar10_{epoch}.pth\")[\"state_dict\"])\n",
    "    return model\n",
    "\n",
    "def save_experiment(model, epoch, config, train_losses, test_losses, train_accuracies,test_accuracies, path):\n",
    "    exp_data = {\n",
    "        \"train_losses\":train_losses,\n",
    "        \"test_losses\":test_losses,\n",
    "        \"train_accuracies\":train_accuracies,\n",
    "        \"test_accuracies\":test_accuracies,\n",
    "        \"epoch\":epoch,\n",
    "    }\n",
    "    exp_name = config[\"exp_name\"]\n",
    "    config_file = path/f\"{exp_name}\"/\"config.json\"\n",
    "    metrics_file = path/f\"{exp_name}\"/\"metrics.json\"\n",
    "    files = [config_file , metrics_file]\n",
    "    for file in files:\n",
    "        if file.exists():\n",
    "            print(f\"{file} exists\")\n",
    "        else:\n",
    "            file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            file.touch()\n",
    "            print(f\"{file} created\")\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(exp_data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    save_checkpoint(model.state_dict(), epoch, path/f\"{exp_name}\")\n",
    "\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    with open(path/f\"{exp_name}\"/\"metrics.json\", 'r') as file:\n",
    "      data = json.load(file)\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    train_accuracies=data[\"train_accuracies\"]\n",
    "    test_accuracies=data[\"test_accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path/exp_name, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, train_accuracies,test_accuracies, epoch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a9206-35fa-4209-ba71-4a1245e80fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config ={\n",
    "        \"img_size\":28,\n",
    "        \"embed_dim\":16, # (PATCH_SIZE ** 2) * IN_CHANNELS\n",
    "        \"patch_size\":4,\n",
    "        \"dropout\":0.01,\n",
    "        \"num_channels\":1,\n",
    "        \"num_heads\":4,\n",
    "        \"num_layers\":8,\n",
    "        \"num_classes\":10,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"exp_name\":\"vit_mnist_40_epoch\",\n",
    "        \"num_epoch\":40\n",
    "    }\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, device):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, train_loader, test_loader, num_epochs):\n",
    "        train_losses, test_losses, train_accuracies, test_accuracies = [], [] , [], []\n",
    "        start = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            ep_start = time.time()\n",
    "            train_loss, train_accuracy = self.train_epoch(train_loader)\n",
    "            test_loss, test_accuracy = self.evaluate(test_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "\n",
    "            ep_end = time.time()\n",
    "            print(f\"Epoch: {epoch}/{num_epochs}, Time : {(ep_end-ep_start):.2f}s\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Train Accuracy : {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "            \n",
    "        save_experiment(self.model, num_epochs, config, train_losses, test_losses, train_accuracies,test_accuracies, EXP_DIR)\n",
    "        end = time.time()\n",
    "        print(f\"Total Training Time : {(end-start):.2f}s\")\n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        correct = 0\n",
    "        running_train_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(tqdm(train_loader, position=0, leave=True)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.type(torch.uint8).to(device)\n",
    "\n",
    "            predictions = self.model(imgs)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            running_train_loss += loss.item() * len(imgs)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "            correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        accuracy = correct / len(train_loader.dataset)\n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        return train_loss, accuracy\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        running_test_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(tqdm(test_loader, position=0, leave=True)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.type(torch.uint8).to(device)\n",
    "\n",
    "            predictions = self.model(imgs)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            running_test_loss += loss.item() * len(imgs)\n",
    "\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "            correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        test_loss = running_test_loss / len(test_loader.dataset)\n",
    "        return test_loss, accuracy\n",
    "            \n",
    "def main():\n",
    "    model = ViT(config)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    trainer = Trainer(model, criterion, optimizer, config[\"device\"])\n",
    "    trainer.train(train_loader, test_loader, config[\"num_epochs\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064f709-b40e-44f6-867f-b0d6d73f3136",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1e08f-70c0-47db-9dbe-7ba2536fab8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize Losses\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = ViT(config)\n",
    "_, train_losses, test_losses, train_accuracies, test_accuracies,_ = load_experiment(model, config[\"exp_name\"], EXPERIMENT_DIR)\n",
    "# Create two subplots of train/test losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(train_losses, label=\"Train loss\")\n",
    "ax1.plot(test_losses, label=\"Test loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax2.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "ax2.plot(test_accuracies, label=\"Test accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc06e8-09cd-4e69-bdce-b542df1bb57c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767e4e6-c97d-45d2-82bb-0f42d92a6212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d9b60-ee0a-4581-bdb0-fc01c1342d11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273a0a8-6879-4228-9017-728d0382bd77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e4e6c-3a4c-451d-b163-5a91c9c12629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509593ce-0d6c-4a99-8225-77b37e51d04d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f3dea5-b17e-461b-b135-2295fd12ef94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c10460-d222-49d3-a78c-b7f6c9119c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea482817-5a1b-42a5-b225-74475488ee15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fbd4e4-3da9-4e03-a4fb-82f3e233e39e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
