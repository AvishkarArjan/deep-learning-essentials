{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ee79e-3fb3-4dc9-84ba-4a46eb7301b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Inspiration : https://github.com/nerminnuraydogan/vision-transformer/blob/main/vision-transformer.ipynb\"\"\"\n",
    "\"\"\"Main Inspiration : https://tintn.github.io/Implementing-Vision-Transformer-from-Scratch/\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dddb1d-1a82-4bd2-8952-fedfc988965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_ROOT = Path(\"/content/drive/MyDrive/research/vision_transformer\")\n",
    "    CHECKPOINT_DIR = DATA_ROOT/\"vit_cifar_checkpoints\"\n",
    "    EXPERIMENT_DIR = DATA_ROOT/\"experiments\"\n",
    "except:\n",
    "    DATA_ROOT = Path.home()/\"Desktop/research\"\n",
    "    CHECKPOINT_DIR = Path.home()/\"Desktop/projects/deep_learning_essentials/vision_transformer/vit_cifar_checkpoints\"\n",
    "    EXPERIMENT_DIR = Path.home()/\"Desktop/projects/deep_learning_essentials/vision_transformer/experiments\"\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    DATA_ROOT.mkdir()\n",
    "    print(\"Created Data dir\")\n",
    "else:\n",
    "    print(\"DATA_ROOT exists at : \", DATA_ROOT)\n",
    "    \n",
    "if not CHECKPOINT_DIR.exists():\n",
    "    CHECKPOINT_DIR.mkdir()\n",
    "    print(\"created CKPT dir\")\n",
    "else:\n",
    "    print(\"ckpt exists at : \", CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eabf10-6a49-4d83-a3b0-2f8813e5e3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DATASET\"\"\"\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATASET = \"cifar10\"\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "if DATASET==\"cifar10\":\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((32,32)),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "train_dataset = datasets.CIFAR10(DATA_ROOT, train=True, download=True, transform= transform)\n",
    "test_dataset = datasets.CIFAR10(DATA_ROOT, train=False, download=True, transform= transform)\n",
    "print(f\"trainset : {len(train_dataset)}, testset : {len(test_dataset)}\")\n",
    "classes = train_dataset.classes\n",
    "\n",
    "# trainset = torch.utils.data.Subset(train_dataset, list(range(5000)))\n",
    "# testset = torch.utils.data.Subset(test_dataset, list(range(1000)))\n",
    "\n",
    "# prepare\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028b1acf-3453-4005-b2b4-a6121fe9ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! du -sh /home/nixocid/desktop/research/cifar-10-batches-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e28b4-76eb-4be8-b615-7c095b513e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VISUALIZE DATA\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, (imgs, labels) in enumerate(train_loader):\n",
    "    img = imgs[0]\n",
    "    plt.imshow(img.T.cpu().numpy())\n",
    "    plt.show()\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383cc94-a6b5-49b0-976a-f58c322aac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MODEL\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.img_size = config[\"img_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        \n",
    "        self.num_patches = (self.img_size//self.patch_size)**2\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        # print(x.shape)\n",
    "        x = x.flatten(2).transpose(1,2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    # Patch Embeddings + (CLS Token + Positional Embeddings )\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,self.embed_dim))\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches+1, self.embed_dim))\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1) # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = x + self.positional_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, attention_head_size,config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.attention_head_size = attention_head_size\n",
    "        self.bias = config[\"bias\"]\n",
    "        \n",
    "        self.query = nn.Linear(self.embed_dim, self.attention_head_size, bias=self.bias)\n",
    "        self.key = nn.Linear(self.embed_dim, self.attention_head_size, bias=self.bias)\n",
    "        self.value = nn.Linear(self.embed_dim, self.attention_head_size, bias=self.bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-1,-2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_scores = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "        attention_out = torch.matmul(attention_scores, v)\n",
    "        \n",
    "        return attention_out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.head_size = self.embed_dim//self.num_heads\n",
    "        self.all_head_size = self.head_size * self.num_heads\n",
    "        self.dropout = config[\"dropout\"]\n",
    "        self.qkv_bias = config[\"bias\"]\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(\n",
    "                self.head_size,\n",
    "                config\n",
    "            ) for _ in range(self.num_heads)\n",
    "        ])\n",
    "\n",
    "        self.attention_mlp = nn.Linear(self.all_head_size, self.embed_dim)\n",
    "        self.out_dropout = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        attention_output = torch.cat([attention_output for attention_output in attention_outputs], dim=-1) #concat attention for each head\n",
    "        attention_output = self.attention_mlp(attention_output)\n",
    "        attention_output = self.out_dropout(attention_output)\n",
    "\n",
    "        return attention_output\n",
    "        \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.fc1 = nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        self.dropout=nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim=config[\"embed_dim\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.mlp = MLP(config)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        attention_output = self.attention(self.layer_norm1(x))\n",
    "        x = x+attention_output\n",
    "        mlp_out = self.mlp(self.layer_norm2(x))\n",
    "        x = x+mlp_out\n",
    "        return x\n",
    "        \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([Block(config) for _ in range(config[\"num_hidden_layers\"])])\n",
    "    def forward(self,x ):\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.img_size = config[\"img_size\"]\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "    \n",
    "        self.embeddings = Embeddings(config)\n",
    "        self.encoder = Encoder(config)\n",
    "    \n",
    "        self.classifier = nn.Linear(self.embed_dim, self.num_classes)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_output = self.embeddings(x)\n",
    "        encoder_output = self.encoder(embedding_output)\n",
    "        classification = self.classifier(encoder_output[:, 0, :])\n",
    "        return classification\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.positional_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.positional_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=0.02,\n",
    "            ).to(module.positional_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=0.02,\n",
    "            ).to(module.cls_token.dtype)\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f53e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UTILS\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_checkpoint(state_dict, epoch, path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Creating folder\")\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_details = {\n",
    "        \"epoch\":epoch,\n",
    "        \"state_dict\": state_dict,\n",
    "    }\n",
    "    torch.save(model_details, f\"{p}/vit_cifar10_{epoch}.pth\")\n",
    "    print(f\"model saved at path : {p}/vit_cifar10_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, epoch):\n",
    "    model.load_state_dict(torch.load(f\"{path}/vit_cifar10_{epoch}.pth\")[\"state_dict\"])\n",
    "    return model\n",
    "\n",
    "def save_experiment(model, epoch, config, train_losses, test_losses, accuracies, path):\n",
    "    exp_data = {\n",
    "        \"train_losses\":train_losses,\n",
    "        \"test_losses\":test_losses,\n",
    "        \"accuracies\":accuracies,\n",
    "        \"epoch\":epoch,\n",
    "    }\n",
    "    exp_name = config[\"exp_name\"]\n",
    "    config_file = path/f\"{exp_name}\"/\"config.json\"\n",
    "    metrics_file = path/f\"{exp_name}\"/\"metrics.json\"\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(exp_data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    save_checkpoint(model.state_dict(), epoch, path/f\"{exp_name}\")\n",
    "\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    data = json.load(path/f\"{exp_name}\"/\"metrics.json\")\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    accuracies=data[\"accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, accuracies, epoch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69acf0c8-0cc0-4c22-9b77-ee4ee48d84b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CONFIG\"\"\"\n",
    "\n",
    "\n",
    "IMG_SIZE=32\n",
    "PATCH_SIZE=4\n",
    "EMBED_DIM=48\n",
    "DROPOUT=0.0\n",
    "NUM_HIDDEN_LAYERS=4\n",
    "NUM_HEADS=4\n",
    "qkv_bias=True \n",
    "HIDDEN_DIM=48*4\n",
    "\n",
    "config = {\n",
    "    \"img_size\":32,\n",
    "    \"patch_size\":4,\n",
    "    \"num_channels\":3,\n",
    "    \"embed_dim\":48,\n",
    "    \"dropout\":0.0,\n",
    "    \"bias\":True,\n",
    "    \"num_heads\":4,\n",
    "    \"hidden_dim\":4*48,\n",
    "    \"num_hidden_layers\":4,\n",
    "    \"num_classes\":10,\n",
    "    \"device\":\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"exp_name\" : 'vit-with-5-epochs',\n",
    "    \"num_epochs\":5,\n",
    "    \"lr\":0.001,\n",
    "    \"save_model_every\" :0\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e86a952-952d-47f6-8264-22aa90dd7d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device, path=CHECKPOINT_DIR):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.path=path\n",
    "\n",
    "    def train(self, train_loader, test_loader, num_epochs, save_model_every_n_epochs=0):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        accuracies = [] \n",
    "\n",
    "        for i in range(num_epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            accuracy, test_loss = self.evaluate(test_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != num_epochs:\n",
    "                print('\\tSave checkpoint at epoch', i+1)\n",
    "                save_checkpoint(self.model.state_dict(), i+1, train_losses, test_losses, accuracies, self.path)\n",
    "\n",
    "        save_experiment(self.model, num_epochs, config, train_losses, test_losses, accuracies, self.path)\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(train_loader):\n",
    "            imgs = imgs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(imgs)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()* len(imgs)\n",
    "\n",
    "        return total_loss / len(train_loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (imgs, labels) in enumerate(test_loader):\n",
    "                imgs = imgs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                predictions = self.model(imgs)\n",
    "                \n",
    "                loss = self.criterion(predictions, labels)\n",
    "                total_loss += loss.item() * len(imgs)\n",
    "\n",
    "                 # Calculate the accuracy\n",
    "                predictions = torch.argmax(predictions, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        avg_loss = total_loss / len(test_loader.dataset)\n",
    "        return accuracy, avg_loss\n",
    "            \n",
    "\n",
    "def main():\n",
    "    save_model_every_n_epochs = config[\"save_model_every\"]\n",
    "    model = ViT(config)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=1e-2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model, optimizer, criterion, device=config[\"device\"])\n",
    "    trainer.train(train_loader, test_loader, config[\"num_epochs\"], save_model_every_n_epochs=save_model_every_n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688956e3-046f-406f-abe2-8e420cb49078",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"TRAINING\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e636df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VIS Loss, ACC\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    data = json.load(path/f\"exp_name\"/\"metrics.json\")\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    accuracies=data[\"accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, accuracies, epoch\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = ViT(config)\n",
    "_, train_losses, test_losses, accuracies,_ = load_experiment(model, \"vit-with-5-epochs\", EXPERIMENT_DIR)\n",
    "\n",
    "# Create two subplots of train/test losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(train_losses, label=\"Train loss\")\n",
    "ax1.plot(test_losses, label=\"Test loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax2.plot(accuracies)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"metrics.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdd8a1c-92ad-441e-aa1d-02a9c1a4714e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
