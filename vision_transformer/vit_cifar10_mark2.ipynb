{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT exists at :  /home/avishkar/Desktop/research\n",
      "ckpt exists at :  /home/avishkar/Desktop/projects/deep_learning_essentials/vision_transformer/vit_cifar_checkpoints\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_ROOT = Path(\"/content/drive/MyDrive/research/vision_transformer\")\n",
    "    CHECKPOINT_DIR = DATA_ROOT/\"vit_cifar_checkpoints\"\n",
    "    EXPERIMENT_DIR = DATA_ROOT/\"experiments\"\n",
    "except:\n",
    "    DATA_ROOT = Path.home()/\"Desktop/research\"\n",
    "    CHECKPOINT_DIR = Path.home()/\"Desktop/projects/deep_learning_essentials/vision_transformer/vit_cifar_checkpoints\"\n",
    "    EXPERIMENT_DIR = Path.home()/\"Desktop/projects/deep_learning_essentials/vision_transformer/experiments\"\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    DATA_ROOT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"Created Data dir\")\n",
    "else:\n",
    "    print(\"DATA_ROOT exists at : \", DATA_ROOT)\n",
    "    \n",
    "if not CHECKPOINT_DIR.exists():\n",
    "    CHECKPOINT_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # CHECKPOINT_DIR.mkdir()/\n",
    "    print(\"created CKPT dir\")\n",
    "else:\n",
    "    print(\"ckpt exists at : \", CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "trainset : 50000, testset : 10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"DATASET\"\"\"\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATASET = \"cifar10\"\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(DATA_ROOT, train=True, download=True, transform= transforms.ToTensor())\n",
    "test_dataset = datasets.CIFAR10(DATA_ROOT, train=False, download=True, transform= transforms.ToTensor())\n",
    "print(f\"trainset : {len(train_dataset)}, testset : {len(test_dataset)}\")\n",
    "classes = train_dataset.classes\n",
    "\n",
    "# trainset = torch.utils.data.Subset(train_dataset, list(range(5000)))\n",
    "# testset = torch.utils.data.Subset(test_dataset, list(range(1000)))\n",
    "\n",
    "# prepare\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"VISUALIZE DATA\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, (imgs, labels) in enumerate(train_loader):\n",
    "    img = imgs[0]\n",
    "    plt.imshow(img.T.cpu().numpy())\n",
    "    plt.show()\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MODEL\"\"\"\n",
    "\"\"\"based on official PyTorch source code based on original paper : https://pytorch.org/vision/main/models/vision_transformer.html\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "config = {\n",
    "    \"img_size\":32,\n",
    "\t\"patch_size\":6,\n",
    "\t\"num_channels\":3,\n",
    "\t\"num_layers\":7,\n",
    "\t\"num_heads\":8,\n",
    "\t\"embed_dim\":768,\n",
    "\t\"mlp_hidden_dim\":4*768,\n",
    "\t\"dropout\":0.0,\n",
    "\t\"num_classes\":10,\n",
    "    \"lr\":0.01,\n",
    "    \"device\":\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"num_epochs\":100,\n",
    "    \"exp_name\":\"vit_cifar10_mark2_100_epochs\",\n",
    "    \"save_model_every\":0\n",
    "}\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.hidden_dim = config[\"mlp_hidden_dim\"]\n",
    "        self.fc1 = nn.Linear(self.embed_dim, self.hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        self.dropout=nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "            \n",
    "        # Attention block\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(self.embed_dim, self.num_heads, dropout=config[\"dropout\"], batch_first=True)\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "\n",
    "        # MLP block\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embed_dim)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, inp: torch.Tensor):\n",
    "        x = self.layer_norm1(inp) # inp as in  input\n",
    "        x, _ = self.self_attention(x,x,x,need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + inp\n",
    "\n",
    "        y = self.layer_norm2(x)\n",
    "        y = self.mlp(y)\n",
    "\n",
    "        return x+y\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, seq_length ,config):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, config[\"embed_dim\"]).normal_(std=0.02))  # from BERT\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        self.embed_dim =  config[\"embed_dim\"]\n",
    "        self.layers = nn.ModuleList([\n",
    "\t\t\tEncoderBlock(config) for _ in range(config[\"num_layers\"])]\n",
    "\t\t\t)\n",
    "        self.layer_norm = nn.LayerNorm(self.embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pos_embedding\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.conv_proj = nn.Conv2d(\n",
    "\t\t\tconfig[\"num_channels\"], \n",
    "\t\t    config[\"embed_dim\"],\n",
    "\t\t\tkernel_size=config[\"patch_size\"],\n",
    "\t\t\tstride=config[\"patch_size\"])\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.img_size = config[\"img_size\"]\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        seq_length = (config[\"img_size\"] // config[\"patch_size\"]) ** 2\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, config[\"embed_dim\"]))\n",
    "        seq_length += 1\n",
    "\n",
    "        self.encoder = Encoder(seq_length, config)\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        \n",
    "        heads_layers[\"pre_logits\"] = nn.Linear(config[\"embed_dim\"], config[\"mlp_hidden_dim\"])\n",
    "        heads_layers[\"act\"] = nn.Tanh()\n",
    "        heads_layers[\"head\"] = nn.Linear(config[\"mlp_hidden_dim\"] ,config[\"num_classes\"])\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "\n",
    "\n",
    "        # Initializing weights\n",
    "\n",
    "        if isinstance(self.conv_proj, nn.Conv2d):\n",
    "            # Init the patchify stem\n",
    "            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n",
    "            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n",
    "            if self.conv_proj.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.bias)\n",
    "        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n",
    "            # Init the last 1x1 conv of the conv stem\n",
    "            nn.init.normal_(\n",
    "                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n",
    "            )\n",
    "            if self.conv_proj.conv_last.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.conv_last.bias)\n",
    "\n",
    "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
    "            fan_in = self.heads.pre_logits.in_features\n",
    "            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n",
    "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
    "\n",
    "        if isinstance(self.heads.head, nn.Linear):\n",
    "            nn.init.zeros_(self.heads.head.weight)\n",
    "            nn.init.zeros_(self.heads.head.bias)\n",
    "\n",
    "\n",
    "    def forward(self,x ):\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        torch._assert(h == self.img_size, f\"Wrong image height! Expected {self.img_size} but got {h}!\")\n",
    "        torch._assert(w == self.img_size, f\"Wrong image width! Expected {self.img_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.embed_dim, n_h * n_w)\n",
    "\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        x = x[:, 0]\n",
    "\n",
    "        x = self.heads(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UTILS\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_checkpoint(state_dict, epoch, path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Creating folder\")\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_details = {\n",
    "        \"epoch\":epoch,\n",
    "        \"state_dict\": state_dict,\n",
    "    }\n",
    "    torch.save(model_details, f\"{p}/vit_cifar10_{epoch}.pth\")\n",
    "    print(f\"model saved at path : {p}/vit_cifar10_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, epoch):\n",
    "    model.load_state_dict(torch.load(f\"{path}/vit_cifar10_{epoch}.pth\")[\"state_dict\"])\n",
    "    return model\n",
    "\n",
    "def save_experiment(model, epoch, config, train_losses, test_losses, accuracies, path):\n",
    "    exp_data = {\n",
    "        \"train_losses\":train_losses,\n",
    "        \"test_losses\":test_losses,\n",
    "        \"accuracies\":accuracies,\n",
    "        \"epoch\":epoch,\n",
    "    }\n",
    "    exp_name = config[\"exp_name\"]\n",
    "    config_file = path/f\"{exp_name}\"/\"config.json\"\n",
    "    metrics_file = path/f\"{exp_name}\"/\"metrics.json\"\n",
    "    files = [config_file , metrics_file]\n",
    "    for file in files:\n",
    "        if file.exists():\n",
    "            print(f\"{file} exists\")\n",
    "        else:\n",
    "            file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            file.touch()\n",
    "            print(f\"{file} created\")\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(exp_data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    save_checkpoint(model.state_dict(), epoch, path/f\"{exp_name}\")\n",
    "\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    with open(path/f\"{exp_name}\"/\"metrics.json\", 'r') as file:\n",
    "      data = json.load(file)\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    accuracies=data[\"accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path/exp_name, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, accuracies, epoch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device, path=CHECKPOINT_DIR):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.path=path\n",
    "        self.exp_dir = EXPERIMENT_DIR\n",
    "\n",
    "    def train(self, train_loader, test_loader, num_epochs, save_model_every_n_epochs=0):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        accuracies = [] \n",
    "\n",
    "        for i in range(num_epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            accuracy, test_loss = self.evaluate(test_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != num_epochs:\n",
    "                print('\\tSave checkpoint at epoch', i+1)\n",
    "                save_checkpoint(self.model.state_dict(), i+1, train_losses, test_losses, accuracies, self.path)\n",
    "\n",
    "        save_experiment(self.model, num_epochs, config, train_losses, test_losses, accuracies, self.exp_dir)\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(train_loader):\n",
    "            imgs = imgs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(imgs)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()* len(imgs)\n",
    "\n",
    "        return total_loss / len(train_loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (imgs, labels) in enumerate(test_loader):\n",
    "                imgs = imgs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                predictions = self.model(imgs)\n",
    "                \n",
    "                loss = self.criterion(predictions, labels)\n",
    "                total_loss += loss.item() * len(imgs)\n",
    "\n",
    "                 # Calculate the accuracy\n",
    "                predictions = torch.argmax(predictions, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        avg_loss = total_loss / len(test_loader.dataset)\n",
    "        return accuracy, avg_loss\n",
    "            \n",
    "\n",
    "def main():\n",
    "    save_model_every_n_epochs = config[\"save_model_every\"]\n",
    "    model = VisionTransformer(config)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=1e-2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model, optimizer, criterion, device=config[\"device\"])\n",
    "    trainer.train(train_loader, test_loader, config[\"num_epochs\"], save_model_every_n_epochs=save_model_every_n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"TRAINING\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 77\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     76\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optimizer, criterion, device\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 77\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_model_every_n_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_model_every_n_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 19\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, test_loader, num_epochs, save_model_every_n_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m accuracies \u001b[38;5;241m=\u001b[39m [] \n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 19\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     accuracy, test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(test_loader)\n\u001b[1;32m     21\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[59], line 39\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     36\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 39\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(predictions, labels)\n\u001b[1;32m     41\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 170\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m batch_class_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_token\u001b[38;5;241m.\u001b[39mexpand(n, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    168\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch_class_token, x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Classifier \"token\" as used by standard language architectures\u001b[39;00m\n\u001b[1;32m    173\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 88\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 88\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(x)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 67\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m inp\n\u001b[1;32m     66\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm2(x)\n\u001b[0;32m---> 67\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m+\u001b[39my\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 38\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[0;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/activation.py:696\u001b[0m, in \u001b[0;36mGELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapproximate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapproximate\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"TRAINING\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VIS Loss, ACC\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    data = json.load(path/f\"exp_name\"/\"metrics.json\")\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    accuracies=data[\"accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, accuracies, epoch\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = ViT(config)\n",
    "_, train_losses, test_losses, accuracies,_ = load_experiment(model, config[\"exp_name\"], EXPERIMENT_DIR)\n",
    "# Create two subplots of train/test losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(train_losses, label=\"Train loss\")\n",
    "ax1.plot(test_losses, label=\"Test loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax2.plot(accuracies)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"metrics.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
