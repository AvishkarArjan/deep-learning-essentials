{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT exists:  /home/avishkar/Desktop/research\n",
      "CHECKPOINT_DIR exists:  /home/avishkar/Desktop/projects/deep_learning_essentials/vision_transformer/vit_cifar_checkpoints\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_ROOT = Path(\"/content/drive/MyDrive/research/vision_transformer\")\n",
    "    CHECKPOINT_DIR = DATA_ROOT/\"vit_cifar_checkpoints\"\n",
    "    EXPERIMENT_DIR = DATA_ROOT/\"experiments\"\n",
    "except:\n",
    "    DATA_ROOT = Path.home()/\"Desktop/research\"\n",
    "    CHECKPOINT_DIR = Path.home()/\"Desktop/projects/deep_learning_essentials/vision_transformer/vit_cifar_checkpoints\"\n",
    "    EXPERIMENT_DIR = Path.home()/\"Desktop/projects/deep_learning_essentials/vision_transformer/experiments\"\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    DATA_ROOT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"DATA_ROOT created: {DATA_ROOT}\")\n",
    "else:\n",
    "    print(\"DATA_ROOT exists: \", DATA_ROOT)\n",
    "    \n",
    "if not CHECKPOINT_DIR.exists():\n",
    "    CHECKPOINT_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"CHECKPOINT_DIR created : {CHECKPOINT_DIR}\")\n",
    "else:\n",
    "    print(\"CHECKPOINT_DIR exists: \", CHECKPOINT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "trainset : 50000, testset : 10000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"DATASET\"\"\"\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATASET = \"cifar10\"\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        # transforms.Resize(32),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=7),\n",
    "        transforms.RandomResizedCrop(size=(32, 32), scale=(0.8, 1.0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(DATA_ROOT, train=True, download=True, transform= transform)\n",
    "test_dataset = datasets.CIFAR10(DATA_ROOT, train=False, download=True, transform= transform)\n",
    "print(f\"trainset : {len(train_dataset)}, testset : {len(test_dataset)}\")\n",
    "classes = train_dataset.classes\n",
    "\n",
    "# prepare\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "\"\"\"VISUALIZE DATA\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, (imgs, labels) in enumerate(train_loader):\n",
    "    img = imgs[0]\n",
    "    plt.imshow(img.T.cpu().numpy())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"https://github.com/chingisooinar/AI_self-driving-car/blob/main/model/SimpleTransformer.py\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "\"\"\"\n",
    "Special Points\n",
    "    - ResNet18 used from Positional Encoding\n",
    "\"\"\"\n",
    "\n",
    "config={\n",
    "    \"img_size\":32,\n",
    "    \"patch_size\":4,\n",
    "    \"seq_len\" : (32//4)**2,\n",
    "    \"dropout\":0.0,\n",
    "}\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.seq_len = config[\"seq_len\"]  #seq_length = (self.img_size // self.patch_size) ** 2 Most probably\n",
    "        self.position_encoder = models.resnet18(pretrained=True)\n",
    "        self.d_model = 512\n",
    "        self.position_embedder = nn.Linear(in_features =1000, out_features = self.d_model, bias=True)\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=4, dropout=0.1)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2, norm=None) #nn.LayerNorm(512)\n",
    "        \n",
    "        self.reduce_combined = nn.Linear(in_features =self.d_model, out_features = 64, bias=True)\n",
    "        \n",
    "        self.classifier = nn.Linear(in_features = 64, out_features = 10, bias=True)\n",
    "        # self.speed_predictor = nn.Linear(in_features = 64, out_features = 1, bias=True)\n",
    "        \n",
    "        \n",
    "    def generate_square_subsequent_mask(self,sz: int):\n",
    "        \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "        return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 3, 32, 32)\n",
    "        x = F.relu(self.position_embedder(self.position_encoder(x)))\n",
    "        x = x.reshape(-1, self.seq_len, self.d_model).permute(1, 0 ,2)\n",
    "        \n",
    "        # attn_mask = self.generate_square_subsequent_mask(x.shape[0]).cuda()\n",
    "        attn_mask = self.generate_square_subsequent_mask(x.shape[0])\n",
    "        \n",
    "        fused_embedding = F.relu(self.transformer_encoder(x, mask=attn_mask))\n",
    "        \n",
    "        fused_embedding = fused_embedding.permute(1, 0, 2)\n",
    "        fused_embedding = fused_embedding.reshape(-1, self.d_model)\n",
    "        reduced = F.relu(self.reduce_combined(fused_embedding))\n",
    "        \n",
    "        angle = self.classifier(reduced)\n",
    "        # speed = self.speed_predictor(reduced)\n",
    "        \n",
    "        return angle\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UTILS\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_checkpoint(state_dict, epoch, path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Creating folder\")\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_details = {\n",
    "        \"epoch\":epoch,\n",
    "        \"state_dict\": state_dict,\n",
    "    }\n",
    "    torch.save(model_details, f\"{p}/vit_cifar10_{epoch}.pth\")\n",
    "    print(f\"model saved at path : {p}/vit_cifar10_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, epoch):\n",
    "    model.load_state_dict(torch.load(f\"{path}/vit_cifar10_{epoch}.pth\")[\"state_dict\"])\n",
    "    return model\n",
    "\n",
    "def save_experiment(model, epoch, config, train_losses, test_losses, accuracies, path):\n",
    "    exp_data = {\n",
    "        \"train_losses\":train_losses,\n",
    "        \"test_losses\":test_losses,\n",
    "        \"accuracies\":accuracies,\n",
    "        \"epoch\":epoch,\n",
    "    }\n",
    "    exp_name = config[\"exp_name\"]\n",
    "    config_file = path/f\"{exp_name}\"/\"config.json\"\n",
    "    metrics_file = path/f\"{exp_name}\"/\"metrics.json\"\n",
    "    files = [config_file , metrics_file]\n",
    "    for file in files:\n",
    "        if file.exists():\n",
    "            print(f\"{file} exists\")\n",
    "        else:\n",
    "            file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            file.touch()\n",
    "            print(f\"{file} created\")\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(exp_data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    save_checkpoint(model.state_dict(), epoch, path/f\"{exp_name}\")\n",
    "\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    with open(path/f\"{exp_name}\"/\"metrics.json\", 'r') as file:\n",
    "      data = json.load(file)\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    accuracies=data[\"accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path/exp_name, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, accuracies, epoch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import time\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, criterion, device, path=CHECKPOINT_DIR):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.device = device\n",
    "        self.path=path\n",
    "        self.exp_dir = EXPERIMENT_DIR\n",
    "\n",
    "    def train(self, train_loader, test_loader, num_epochs, save_model_every_n_epochs=0):\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        accuracies = [] \n",
    "        start = time.time()\n",
    "        for i in range(num_epochs):\n",
    "            epo_start = time.time()\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            accuracy, test_loss = self.evaluate(test_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            accuracies.append(accuracy)\n",
    "            ep_end = time.time()\n",
    "            print(f\"Epoch: {i+1}, Train loss: {train_loss:.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}, Time: {(ep_end-epo_start):.4f}s\")\n",
    "            if save_model_every_n_epochs > 0 and (i+1) % save_model_every_n_epochs == 0 and i+1 != num_epochs:\n",
    "                print('\\tSave checkpoint at epoch', i+1)\n",
    "                save_checkpoint(self.model.state_dict(), i+1, train_losses, test_losses, accuracies, self.path)\n",
    "        end = time.time()\n",
    "        print(f\"Total training time : {end- start}s\")\n",
    "        save_experiment(self.model, num_epochs, config, train_losses, test_losses, accuracies, self.exp_dir)\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(train_loader):\n",
    "            imgs = imgs.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(imgs)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()* len(imgs)\n",
    "            break\n",
    "\n",
    "        return total_loss / len(train_loader.dataset)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for i, (imgs, labels) in enumerate(test_loader):\n",
    "                imgs = imgs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                predictions = self.model(imgs)\n",
    "                \n",
    "                loss = self.criterion(predictions, labels)\n",
    "                total_loss += loss.item() * len(imgs)\n",
    "\n",
    "                 # Calculate the accuracy\n",
    "                predictions = torch.argmax(predictions, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "                break\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        avg_loss = total_loss / len(test_loader.dataset)\n",
    "        return accuracy, avg_loss\n",
    "         \n",
    "config = {\n",
    "    \"img_size\":32,\n",
    "\t\"patch_size\":4,\n",
    "    \"seq_len\":32,\n",
    "    # \"seq_len\":(32//4)**2,\n",
    "\t\"num_channels\":3,\n",
    "\t\"num_layers\":7,\n",
    "\t\"num_heads\":8,\n",
    "\t\"embed_dim\":768,\n",
    "\t\"mlp_hidden_dim\":4*768,\n",
    "\t\"dropout\":0.0,\n",
    "\t\"num_classes\":10,\n",
    "    \"lr\":0.01,\n",
    "    \"device\":\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"num_epochs\":100,\n",
    "    \"exp_name\":\"vit_cifar10_mark2_100_epochs\",\n",
    "    \"save_model_every\":0\n",
    "}   \n",
    "\n",
    "def main():\n",
    "    save_model_every_n_epochs = config[\"save_model_every\"]\n",
    "    model = VisionTransformer(config)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=1e-2)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model, optimizer, criterion, device=config[\"device\"])\n",
    "    trainer.train(train_loader, test_loader, config[\"num_epochs\"], save_model_every_n_epochs=save_model_every_n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avishkar/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/avishkar/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/avishkar/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512])\n",
      "torch.Size([32, 512])\n",
      "Epoch: 1, Train loss: 0.0015, Test loss: 0.0084, Accuracy: 0.0004\n",
      "/home/avishkar/Desktop/projects/deep_learning_essentials/vision_transformer/experiments/vit_cifar10_mark2_100_epochs/config.json created\n",
      "/home/avishkar/Desktop/projects/deep_learning_essentials/vision_transformer/experiments/vit_cifar10_mark2_100_epochs/metrics.json created\n",
      "model saved at path : /home/avishkar/Desktop/projects/deep_learning_essentials/vision_transformer/experiments/vit_cifar10_mark2_100_epochs/vit_cifar10_100.pth\n"
     ]
    }
   ],
   "source": [
    "\"\"\"TRAINING\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ViT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 19\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mdef load_experiment(model ,exp_name, path):\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    data = json.load(path/f\"exp_name\"/\"metrics.json\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    return model, train_losses, test_losses, accuracies, epoch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mViT\u001b[49m(config)\n\u001b[1;32m     20\u001b[0m _, train_losses, test_losses, accuracies,_ \u001b[38;5;241m=\u001b[39m load_experiment(model, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_name\u001b[39m\u001b[38;5;124m\"\u001b[39m], EXPERIMENT_DIR)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Create two subplots of train/test losses and accuracies\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ViT' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"VIS Loss, ACC\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    data = json.load(path/f\"exp_name\"/\"metrics.json\")\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    accuracies=data[\"accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, accuracies, epoch\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = ViT(config)\n",
    "_, train_losses, test_losses, accuracies,_ = load_experiment(model, config[\"exp_name\"], EXPERIMENT_DIR)\n",
    "# Create two subplots of train/test losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(train_losses, label=\"Train loss\")\n",
    "ax1.plot(test_losses, label=\"Test loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax2.plot(accuracies)\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"metrics.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
