{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_ROOT exists:  /home/avishkar/Desktop/research\n",
      "EXP_DIR created: /home/avishkar/Desktop/research/experiments\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    DATA_ROOT = Path(\"/content/drive/MyDrive/research/vision_transformer\")\n",
    "    EXP_DIR = DATA_ROOT/\"experiments\"\n",
    "except:\n",
    "    DATA_ROOT = Path.home()/\"Desktop/research\"\n",
    "    EXP_DIR = Path.home()/\"Desktop/research/experiments\"\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    DATA_ROOT.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"DATA_ROOT created: {DATA_ROOT}\")\n",
    "else:\n",
    "    print(\"DATA_ROOT exists: \", DATA_ROOT)\n",
    "    \n",
    "if not EXP_DIR.exists():\n",
    "    EXP_DIR.parent.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"EXP_DIR created: {EXP_DIR}\")\n",
    "else:\n",
    "    print(\"EXP_DIR exists: \", EXP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "trainset : 50000, testset : 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.97484297..0.8323338].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"DATASET\"\"\"\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "DATASET = \"cifar10\"\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 32 \n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        # transforms.Resize(32),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=7),\n",
    "        transforms.RandomResizedCrop(size=(32, 32), scale=(0.8, 1.0)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(DATA_ROOT, train=True, download=True, transform= transform)\n",
    "test_dataset = datasets.CIFAR10(DATA_ROOT, train=False, download=True, transform= transform)\n",
    "print(f\"trainset : {len(train_dataset)}, testset : {len(test_dataset)}\")\n",
    "classes = train_dataset.classes\n",
    "\n",
    "# prepare\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "\"\"\"VISUALIZE DATA\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i, (imgs, labels) in enumerate(train_loader):\n",
    "    img = imgs[0]\n",
    "    plt.imshow(img.T.cpu().numpy())\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.img_size = config[\"img_size\"]\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_patches = (self.img_size // self.patch_size) **2\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.patcher = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=self.num_channels,\n",
    "                out_channels=self.embed_dim,\n",
    "                kernel_size=self.patch_size,\n",
    "                stride=self.patch_size,\n",
    "            ),\n",
    "            nn.Flatten(2)\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(\n",
    "            torch.randn(size=(1, 1, self.embed_dim)),\n",
    "            requires_grad=True\n",
    "            )\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.randn(size=(1, self.num_patches+1, self.embed_dim)),\n",
    "            requires_grad=True\n",
    "            )\n",
    "        self.dropout = nn.Dropout(config[\"dropout\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        \n",
    "        x = self.patcher(x).permute(0, 2, 1)\n",
    "        x = torch.cat([cls_token,x ], dim=1)\n",
    "        x = self.position_embeddings + x\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.num_heads = config[\"num_heads\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        \n",
    "        self.embeddings = PatchEmbeddings(config)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.embed_dim,\n",
    "            nhead=self.num_heads,\n",
    "            dropout=config[\"dropout\"],\n",
    "            activation = \"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.encoder_blocks = nn.TransformerEncoder(\n",
    "            self.encoder_layer,\n",
    "            num_layers = config[\"num_layers\"]\n",
    "        )\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(self.embed_dim),\n",
    "            nn.Linear(self.embed_dim, self.num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.encoder_blocks(x)\n",
    "        x = self.mlp_head(x[:, 0, :]) # apply MLP on the CLS token only\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"UTILS\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def save_checkpoint(state_dict, epoch, path):\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        print(\"Creating folder\")\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model_details = {\n",
    "        \"epoch\":epoch,\n",
    "        \"state_dict\": state_dict,\n",
    "    }\n",
    "    torch.save(model_details, f\"{p}/vit_cifar10_{epoch}.pth\")\n",
    "    print(f\"model saved at path : {p}/vit_cifar10_{epoch}.pth\")\n",
    "\n",
    "\n",
    "def load_pretrained(model, path, epoch):\n",
    "    model.load_state_dict(torch.load(f\"{path}/vit_cifar10_{epoch}.pth\")[\"state_dict\"])\n",
    "    return model\n",
    "\n",
    "def save_experiment(model, epoch, config, train_losses, test_losses, train_accuracies,test_accuracies, path):\n",
    "    exp_data = {\n",
    "        \"train_losses\":train_losses,\n",
    "        \"test_losses\":test_losses,\n",
    "        \"train_accuracies\":train_accuracies,\n",
    "        \"test_accuracies\":test_accuracies,\n",
    "        \"epoch\":epoch,\n",
    "    }\n",
    "    exp_name = config[\"exp_name\"]\n",
    "    config_file = path/f\"{exp_name}\"/\"config.json\"\n",
    "    metrics_file = path/f\"{exp_name}\"/\"metrics.json\"\n",
    "    files = [config_file , metrics_file]\n",
    "    for file in files:\n",
    "        if file.exists():\n",
    "            print(f\"{file} exists\")\n",
    "        else:\n",
    "            file.parent.mkdir(parents=True, exist_ok=True)\n",
    "            file.touch()\n",
    "            print(f\"{file} created\")\n",
    "\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config, f, sort_keys=True, indent=4)\n",
    "    with open(metrics_file, \"w\") as f:\n",
    "        json.dump(exp_data, f, sort_keys=True, indent=4)\n",
    "\n",
    "    save_checkpoint(model.state_dict(), epoch, path/f\"{exp_name}\")\n",
    "\n",
    "def load_experiment(model ,exp_name, path):\n",
    "    with open(path/f\"{exp_name}\"/\"metrics.json\", 'r') as file:\n",
    "      data = json.load(file)\n",
    "    train_losses=data[\"train_losses\"]\n",
    "    test_losses=data[\"test_losses\"]\n",
    "    train_accuracies=data[\"train_accuracies\"]\n",
    "    test_accuracies=data[\"test_accuracies\"]\n",
    "    epoch=data[\"epoch\"]\n",
    "\n",
    "    model = load_pretrained(model, path/exp_name, epoch)\n",
    "\n",
    "    return model, train_losses, test_losses, train_accuracies,test_accuracies, epoch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "config ={\n",
    "        \"img_size\":32,\n",
    "        \"embed_dim\":16*3, # (PATCH_SIZE ** 2) * IN_CHANNELS\n",
    "        \"patch_size\":4,\n",
    "        \"dropout\":0.01,\n",
    "        \"num_channels\":3,\n",
    "        \"num_heads\":4,\n",
    "        \"num_layers\":8,\n",
    "        \"num_classes\":10,\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        \"exp_name\":\"vit_cifar_mark2_20_epoch\",\n",
    "        \"num_epoch\":20\n",
    "    }\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, criterion, optimizer, device):\n",
    "        self.model = model.to(device)\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, train_loader, test_loader, num_epochs):\n",
    "        train_losses, test_losses, train_accuracies, test_accuracies = [], [] , [], []\n",
    "        start = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            ep_start = time.time()\n",
    "            train_loss, train_accuracy = self.train_epoch(train_loader)\n",
    "            test_loss, test_accuracy = self.evaluate(test_loader)\n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            test_accuracies.append(test_accuracy)\n",
    "\n",
    "            ep_end = time.time()\n",
    "            print(f\"Epoch: {epoch}/{num_epochs}, Time : {(ep_end-ep_start):.2f}s\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}\")\n",
    "            print(f\"Train Accuracy : {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "            \n",
    "        save_experiment(self.model, num_epochs, config, train_losses, test_losses, train_accuracies,test_accuracies, EXP_DIR)\n",
    "        end = time.time()\n",
    "        print(f\"Total Training Time : {(end-start):.2f}s\")\n",
    "            \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        correct = 0\n",
    "        running_train_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(tqdm(train_loader, position=0, leave=True)):\n",
    "            imgs = imgs.float().to(self.device)\n",
    "            labels = labels.type(torch.uint8).to(self.device)\n",
    "\n",
    "            predictions = self.model(imgs)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            running_train_loss += loss.item() * len(imgs)\n",
    "            print(loss.item())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "            correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        accuracy = correct / len(train_loader.dataset)\n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "        return train_loss, accuracy\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, test_loader):\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        running_test_loss = 0\n",
    "        for i, (imgs, labels) in enumerate(tqdm(test_loader, position=0, leave=True)):\n",
    "            imgs = imgs.float().to(self.device)\n",
    "            labels = labels.type(torch.uint8).to(self.device)\n",
    "\n",
    "            predictions = self.model(imgs)\n",
    "            loss = self.criterion(predictions, labels)\n",
    "            running_test_loss += loss.item() * len(imgs)\n",
    "\n",
    "            predictions = torch.argmax(predictions, dim=1)\n",
    "            correct += torch.sum(predictions == labels).item()\n",
    "\n",
    "        accuracy = correct / len(test_loader.dataset)\n",
    "        test_loss = running_test_loss / len(test_loader.dataset)\n",
    "        return test_loss, accuracy\n",
    "            \n",
    "def main():\n",
    "    model = ViT(config)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    trainer = Trainer(model, criterion, optimizer, config[\"device\"])\n",
    "    trainer.train(train_loader, test_loader, config[\"num_epoch\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avishkar/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "  0%|          | 0/1563 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1563 [00:00<14:48,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.423978567123413\n",
      "2.39823055267334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1563 [00:01<13:02,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4049675464630127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1563 [00:01<12:10,  2.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5077407360076904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1563 [00:02<11:10,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.392368793487549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1563 [00:02<10:35,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3153433799743652\n",
      "2.2851364612579346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1563 [00:03<10:21,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.436081647872925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1563 [00:03<10:25,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3220789432525635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1563 [00:04<10:22,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.356605052947998\n",
      "2.28776478767395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1563 [00:04<10:35,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2842767238616943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1563 [00:05<11:24,  2.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"TRAINING\"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 95\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     94\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, criterion, optimizer, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 95\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 30\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_loader, test_loader, num_epochs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     29\u001b[0m     ep_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 30\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(test_loader)\n\u001b[1;32m     32\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[30], line 54\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m     51\u001b[0m imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     52\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 54\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(predictions, labels)\n\u001b[1;32m     56\u001b[0m running_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(imgs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[28], line 69\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     68\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(x)\n\u001b[0;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_head(x[:, \u001b[38;5;241m0\u001b[39m, :]) \u001b[38;5;66;03m# apply MLP on the CLS token only\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:415\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    412\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 415\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    418\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:747\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_first:\n\u001b[1;32m    746\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m--> 747\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    749\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, src_mask, src_key_padding_mask, is_causal\u001b[38;5;241m=\u001b[39mis_causal))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/transformer.py:765\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._ff_block\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 765\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"TRAINING\"\"\"\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ViT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 19\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mdef load_experiment(model ,exp_name, path):\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    data = json.load(path/f\"exp_name\"/\"metrics.json\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    return model, train_losses, test_losses, accuracies, epoch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mViT\u001b[49m(config)\n\u001b[1;32m     20\u001b[0m _, train_losses, test_losses, accuracies,_ \u001b[38;5;241m=\u001b[39m load_experiment(model, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_name\u001b[39m\u001b[38;5;124m\"\u001b[39m], EXPERIMENT_DIR)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Create two subplots of train/test losses and accuracies\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ViT' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Visualize Losses\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = ViT(config)\n",
    "_, train_losses, test_losses, train_accuracies, test_accuracies,_ = load_experiment(model, config[\"exp_name\"], EXP_DIR)\n",
    "# Create two subplots of train/test losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax1.plot(train_losses, label=\"Train loss\")\n",
    "ax1.plot(test_losses, label=\"Test loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax2.plot(train_accuracies, label=\"Training Accuracy\")\n",
    "ax2.plot(test_accuracies, label=\"Test accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "plt.savefig(\"metrics.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
